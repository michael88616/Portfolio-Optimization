{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the Optimization Problem\n",
    "\n",
    "    Majority of the this notebook is to formulate the problem. The main part is described as follows\n",
    "\n",
    "### Objectives \n",
    "\n",
    "    options:\n",
    "    \n",
    "    * minimize distance between target portfolio and optimal portfolio (with constraints) \n",
    "    \n",
    "    * minimize turnover\n",
    "    \n",
    "    * minimize trading cost\n",
    "        \n",
    "### Constraints\n",
    "\n",
    "    constraints should be prioritized to achieve as many constraints as possible\n",
    "    \n",
    "    Basic Constraints:\n",
    "    \n",
    "        1, Long MKV is within 45000000 and 55000000\n",
    "        2, Net dollar exposure is within 0.3 and 0.4\n",
    "        3, Net beta exposure is within 0.15 and 0.25\n",
    "        4, Max single position weight is 0.05\n",
    "        \n",
    "    Liquidity Constraint:    \n",
    "    \n",
    "        5, Max liquidity is 10% of stock 20D moving average volume\n",
    "    \n",
    "    Sub industry beta exposure constraint\n",
    "    \n",
    "        6, Sub industry beta exposure is within -0.005 and 0.005\n",
    "\n",
    "### Solve Problem\n",
    "\n",
    "    Problem(cp.Minimize(objective),  constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the Notebook\n",
    "\n",
    "### 1. Preprocessing data to covert signals to target portfolio\n",
    "\n",
    "\n",
    "### 2. Define all variables (used in cvxpy solver) in numpy array\n",
    "\n",
    "\n",
    "### 3. Solve problem portfolio by portfolio iteratively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@phrag10\n",
    "\n",
    "conda activate /home/researcher/anaconda3/envs/mc_py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cvxpy as cp\n",
    "import warnings\n",
    "import xlrd\n",
    "plt.rcParams[\"figure.figsize\"] = [15,8]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "dir_farm = r'/media/farmshare/'\n",
    "dir_farm2 = r'/media/farmshare2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pool_dump_data (list_data):\n",
    "    \n",
    "    dict_data = {}\n",
    "    \n",
    "    dir_pool_dump_2000_2015 = dir_farm2 + '_PoolDump/RUSSELL3000_CONSOLIDATED/DATA-625/20190415/2000_2015/LATEST/'\n",
    "    dir_pool_dump_2016_2018 = dir_farm2 + '_PoolDump/RUSSELL3000_CONSOLIDATED/DATA-625/20190415/2016_2018/LATEST/'\n",
    "    \n",
    "    for _data in list_data:\n",
    "        \n",
    "        print (_data)\n",
    "        \n",
    "        df_1 = pd.read_csv(dir_pool_dump_2000_2015 + _data + '.csv', index_col=0)\n",
    "        df_2 = pd.read_csv(dir_pool_dump_2016_2018 + _data + '.csv', index_col=0)\n",
    "        df_3 = (df_1.append(df_2))\n",
    "        df_3 = df_3.loc[~df_3.index.duplicated(keep='first')]\n",
    "        \n",
    "        dict_data[_data] = df_3\n",
    "    \n",
    "    return dict_data\n",
    "\n",
    "def fetch_extra_dump_data (dir_dump, list_data):\n",
    "    \n",
    "    dict_data = {}\n",
    "        \n",
    "    for _data in list_data:\n",
    "        \n",
    "        print (_data)\n",
    "                \n",
    "        dict_data[_data] = pd.read_csv(dir_dump + _data + '.csv', index_col=0)\n",
    "    \n",
    "    return dict_data\n",
    "\n",
    "def check_constraints(df_daily_stock_variable):\n",
    "\n",
    "    # 1, 2, 3\n",
    "    \n",
    "    df_daily_stock_variable['order_mkv'] = df_daily_stock_variable['order_qty'] * df_daily_stock_variable['close_price']\n",
    "    df_port = df_daily_stock_variable.groupby(['side'])[['holding_mkv', 'holding_mkv_beta_adj','order_mkv']].sum()\n",
    "\n",
    "    df_port.loc['Net'] = df_daily_stock_variable.sum(axis=0)\n",
    "    \n",
    "    df_port ['target_beta_exposure'] = df_port['holding_mkv_beta_adj'] / df_port['holding_mkv']['Long']\n",
    "    df_port ['target_exposure'] = df_port['holding_mkv'] / df_port['holding_mkv']['Long']\n",
    "\n",
    "    # 4\n",
    "\n",
    "    df_sibind_beta_exposure = df_daily_stock_variable.groupby(['gics'])['holding_mkv_beta_adj'].sum() \\\n",
    "                                / df_port['holding_mkv']['Long']\n",
    "    \n",
    "    subind_beta_exposure_range = (df_sibind_beta_exposure.min(), df_sibind_beta_exposure.max())\n",
    "    \n",
    "    # 5\n",
    "\n",
    "    max_weight_long = (df_daily_stock_variable['holding_mkv'][df_daily_stock_variable['holding_mkv']>0] \\\n",
    "                        / (df_daily_stock_variable['holding_mkv'][df_daily_stock_variable['holding_mkv']>0].sum())).max()\n",
    "    \n",
    "    max_weight_short = (df_daily_stock_variable['holding_mkv'][df_daily_stock_variable['holding_mkv']<0] \\\n",
    "                        / (df_daily_stock_variable['holding_mkv'][df_daily_stock_variable['holding_mkv']<0].sum())).max()\n",
    "    \n",
    "    max_weight = max(max_weight_long, max_weight_short)\n",
    "    \n",
    "    return df_port, subind_beta_exposure_range, max_weight\n",
    "\n",
    "def get_subind_constr_abs (df_daily_stock_variable, \n",
    "                           x, ls, beta, cap_l, subind_range):\n",
    "    \n",
    "    list_constr = []\n",
    "    \n",
    "    df_gics_count = df_daily_stock_variable.reset_index().groupby(['gics']).sum()[['ind_l','ind_s']]    \n",
    "    list_gics = list(set(df_gics_count[df_gics_count.sum(axis=1)>0].index))\n",
    "    \n",
    "    for _gics in list_gics:\n",
    "        \n",
    "#         print (_gics)\n",
    "        \n",
    "        subind_ind = ((df_daily_stock_variable['gics'] == _gics) + 0).values     \n",
    "        \n",
    "        ub_cap = subind_range * cap_l\n",
    "        lb_cap = -1 * subind_range * cap_l\n",
    " \n",
    "        constr = [cp.sum(x * (beta * subind_ind * ls)) <= ub_cap,\n",
    "                  cp.sum(x * (beta * subind_ind * ls)) >= lb_cap]\n",
    "        \n",
    "        list_constr.extend(constr)\n",
    "        \n",
    "    return list_constr\n",
    "\n",
    "def int_clip(n, smallest, largest): \n",
    "    \n",
    "    return max(smallest, min(n, largest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_signals = dir_farm + '_JIRA/PAP/PAP-32/3.TradingSignals/output/'\n",
    "signal_file = 'SI_TRADE_SIGNAL.csv'\n",
    "dir_out = dir_farm2 + '/Research/mchen/reports/20190715_opt/'\n",
    "dir_pooldump = dir_farm2 + '_PoolDump/RUSSELL3000_CONSOLIDATED/DATA-625/20190415/'\n",
    "\n",
    "mkv_range_long = (45000000, 55000000)\n",
    "net_dollar_range = (0.3, 0.4)\n",
    "net_beta_range = (0.15, 0.25)\n",
    "max_single_position_w = 0.05\n",
    "liquidity_mul = 0.1\n",
    "subind_range = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing data to covert signals to target portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_barraBetaR3K_Fast\n",
      "_closeF\n",
      "_subIndustryF\n",
      "_volumeF\n"
     ]
    }
   ],
   "source": [
    "dict_data = fetch_extra_dump_data(dir_farm + '_JIRA/PAP/PAP-32/3.TradingSignals/ExtraDump/', \n",
    "                                  ['_barraBetaR3K_Fast','_closeF','_subIndustryF','_volumeF'])\n",
    "dict_data['pre_20d_turnover_ma'] = (dict_data['_closeF'].mul(dict_data['_volumeF'])).rolling(20, min_periods = 1).median().shift(1)\n",
    "dict_data['liquidity'] = (liquidity_mul * dict_data['pre_20d_turnover_ma'].div(dict_data['_closeF'])).fillna(0).astype(int)\n",
    "df_gics = pd.read_excel(dir_farm2 + '_PoolDump/GICS CODE Label Jun2019.xlsx', header = None)\n",
    "df_gics['len'] = df_gics[0].apply(lambda x: len(str(x))) \n",
    "df_gics = df_gics[df_gics['len'] == 8]\n",
    "dict_gics = dict(zip(df_gics[1], df_gics[0]))\n",
    "dict_data['si_release_dates'] = pd.read_csv(dir_out + 'si_release_dates.csv', index_col = 0).loc['2010-01-01':'2015-12-15']['release_date'].tolist()\n",
    "dict_data['si_release_dates'][dict_data['si_release_dates'].index('2011-09-10')] = '2011-09-12'\n",
    "dict_data['_pool'] = pd.read_csv(dir_signals + '_R3K_BASE_INDICATOR.csv', index_col = 0).loc['2010-01-01':'2015-12-31']\n",
    "dict_data['cost'] = pd.read_csv(dir_pooldump + 'CostModel/10000.0_cost_total_enter.csv', index_col = 0).loc['2010-01-01':'2015-12-31']\n",
    "dict_data['borrow_fee'] = pd.read_csv(dir_pooldump + '2000_2015/LATEST/_phraBorrowingCost.csv', index_col = 0).loc['2010-01-01':'2015-12-31']\n",
    "\n",
    "dict_data['si_release_dates'] = pd.read_csv(dir_out + 'si_release_dates.csv', index_col = 0).loc['2016-01-01':'2018-12-26']['release_date'].tolist()\n",
    "dict_data['_pool'] = pd.read_csv(dir_signals + '_R3K_BASE_INDICATOR.csv', index_col = 0).loc['2016-01-01':'2018-12-31']\n",
    "dict_data['cost'] = pd.read_csv(dir_pooldump + 'CostModel/10000.0_cost_total_enter.csv', index_col = 0).loc['2016-01-01':'2018-12-27']\n",
    "dict_data['borrow_fee'] = pd.read_csv(dir_pooldump + '2016_2018/LATEST/_phraBorrowingCost.csv', index_col = 0).loc['2016-01-01':'2018-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signal = pd.read_csv(dir_signals + signal_file, index_col = 0).dropna(how='all')\n",
    "df_signal_long_ind = ((df_signal.multiply(dict_data['_pool']).rank(axis = 1, ascending = False) <= 1000) + 0)\n",
    "df_signal_short_ind = ((df_signal.multiply(dict_data['_pool']).rank(axis = 1, ascending = True) <= 1000) + 0)\n",
    "df_signal_th_sign = df_signal_long_ind.add(-1 * df_signal_short_ind, fill_value = 0)\n",
    "\n",
    "df_signal_th_sign_cum = df_signal_th_sign.rolling(1).sum()\n",
    "df_scaling_factor_long = pd.read_csv(dir_signals + '_LONG_SCALE_FACTOR_Beta1_McapNaN.csv', index_col = 0).dropna(how='all')\n",
    "df_scaling_factor_short = pd.read_csv(dir_signals + '_SHORT_SCALE_FACTOR_Beta1_McapNaN.csv', index_col = 0).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dates =  dict_data['si_release_dates']\n",
    "list_dates.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define all variables (used in cvxpy solver) in numpy array\n",
    "### 3. Solve problem portfolio by portfolio iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-27\n",
      "optimal\n",
      "2016-02-09\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2016-02-24\n",
      "optimal\n",
      "2016-03-09\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2016-03-24\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2016-04-11\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2016-04-26\n",
      "optimal\n",
      "2016-05-10\n",
      "optimal\n",
      "2016-05-24\n",
      "optimal\n",
      "2016-06-09\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2016-06-24\n",
      "optimal\n",
      "2016-07-12\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2016-07-26\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2016-08-09\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2016-08-24\n",
      "optimal\n",
      "2016-09-12\n",
      "optimal\n",
      "2016-09-26\n",
      "optimal\n",
      "2016-10-11\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2016-10-25\n",
      "optimal\n",
      "2016-11-09\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2016-11-25\n",
      "optimal\n",
      "2016-12-09\n",
      "optimal\n",
      "2016-12-27\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2017-01-11\n",
      "optimal\n",
      "2017-01-26\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2017-02-09\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2017-02-27\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2017-03-09\n",
      "optimal\n",
      "2017-03-24\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2017-04-12\n",
      "optimal\n",
      "2017-04-25\n",
      "optimal\n",
      "2017-05-09\n",
      "optimal\n",
      "2017-05-24\n",
      "optimal\n",
      "2017-06-09\n",
      "optimal\n",
      "2017-06-26\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2017-07-12\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2017-07-25\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2017-08-09\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2017-08-24\n",
      "optimal\n",
      "2017-09-12\n",
      "optimal\n",
      "2017-09-26\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2017-10-10\n",
      "optimal\n",
      "2017-10-24\n",
      "optimal\n",
      "2017-11-09\n",
      "optimal\n",
      "2017-11-27\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2017-12-11\n",
      "optimal\n",
      "2017-12-27\n",
      "optimal\n",
      "2018-01-10\n",
      "optimal\n",
      "2018-01-24\n",
      "optimal\n",
      "2018-02-09\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2018-02-27\n",
      "optimal\n",
      "2018-03-09\n",
      "optimal\n",
      "2018-03-26\n",
      "optimal\n",
      "2018-04-10\n",
      "optimal\n",
      "2018-04-24\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2018-05-09\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2018-05-24\n",
      "optimal\n",
      "2018-06-11\n",
      "optimal\n",
      "2018-06-26\n",
      "optimal\n",
      "2018-07-11\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2018-07-24\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2018-08-09\n",
      "optimal\n",
      "2018-08-24\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2018-09-12\n",
      "optimal\n",
      "2018-09-25\n",
      "optimal\n",
      "2018-10-09\n",
      "optimal\n",
      "2018-10-24\n",
      "optimal\n",
      "2018-11-09\n",
      "optimal\n",
      "2018-11-27\n",
      "optimal_inaccurate\n",
      "optimal\n",
      "2018-12-11\n",
      "optimal\n",
      "2018-12-26\n",
      "optimal\n"
     ]
    }
   ],
   "source": [
    "dict_daily_order = {}\n",
    "df_status = pd.DataFrame(index = list_dates, columns = ['cons1','cons2','cons3'])\n",
    "\n",
    "for _date in list_dates:\n",
    "\n",
    "    print (_date)\n",
    "\n",
    "    # get stock variables\n",
    "\n",
    "    df_daily_stock_variable = pd.DataFrame(index = df_signal_th_sign_cum.columns,\n",
    "                                           columns = ['cum_signal','tar_w','cur_hold_qty','ind_l','ind_s','sf_l','sf_s',\n",
    "                                                      'ind_pool','gics','beta','close_price','liquidity'])\n",
    "\n",
    "    df_daily_stock_variable['cum_signal'] = df_signal_th_sign_cum.loc[_date].fillna(0)\n",
    "    df_daily_stock_variable['ind_l'] = (df_daily_stock_variable['cum_signal'] > 0) + 0\n",
    "    df_daily_stock_variable['sf_l'] = df_scaling_factor_long.loc[_date]\n",
    "    df_daily_stock_variable['ind_s'] = (df_daily_stock_variable['cum_signal'] < 0) + 0\n",
    "    df_daily_stock_variable['sf_s'] = df_scaling_factor_short.loc[_date]\n",
    "    df_daily_stock_variable['ind_pool'] = 1 # place holder to apply filter\n",
    "\n",
    "    df_signal_l = df_daily_stock_variable['cum_signal'][df_daily_stock_variable['ind_l'] * df_daily_stock_variable['ind_pool'] == 1] \\\n",
    "                    * df_daily_stock_variable['sf_l']\n",
    "    df_signal_s = df_daily_stock_variable['cum_signal'][df_daily_stock_variable['ind_s'] * df_daily_stock_variable['ind_pool'] == 1] \\\n",
    "                    * df_daily_stock_variable['sf_s']\n",
    "\n",
    "    df_daily_stock_variable['tar_w'] = ((df_signal_l/df_signal_l.sum()) \\\n",
    "                                        .add(-1*df_signal_s/df_signal_s.sum(), fill_value = 0))\n",
    "    df_daily_stock_variable['tar_w'] = df_daily_stock_variable['tar_w'].fillna(0).round(6)\n",
    "\n",
    "    if _date == list_dates[0]:\n",
    "\n",
    "        df_daily_stock_variable['cur_hold_qty'] = 0\n",
    "\n",
    "    else:\n",
    "\n",
    "        pre_date = list_dates[list_dates.index(_date) - 1]\n",
    "        df_daily_stock_variable['cur_hold_qty'] = (dict_daily_order[pre_date]['cur_hold_qty'] + \\\n",
    "                                                  dict_daily_order[pre_date]['order_qty']).fillna(0)\n",
    "\n",
    "    df_daily_stock_variable['gics'] = dict_data['_subIndustryF'].iloc[0].fillna(0).astype(int)\n",
    "    df_daily_stock_variable['beta'] = dict_data['_barraBetaR3K_Fast'].loc[_date].fillna(1)\n",
    "    df_daily_stock_variable['close_price'] = dict_data['_closeF'].fillna(method='ffill').loc[_date].fillna(0)\n",
    "    df_daily_stock_variable['liquidity'] = dict_data['liquidity'].loc[_date].fillna(0)\n",
    "    df_daily_stock_variable['cost'] = (dict_data['cost'].loc[_date].fillna(0)/10000).round(4)\n",
    "    df_daily_stock_variable['borrow_fee'] = dict_data['borrow_fee'].loc[_date].fillna(0) * 10 /(100 * 250)\n",
    "    df_daily_stock_variable = df_daily_stock_variable.fillna(0)\n",
    "    df_daily_stock_variable = df_daily_stock_variable[(df_daily_stock_variable['cur_hold_qty'] != 0) \\\n",
    "                                                      | (df_daily_stock_variable['tar_w'] != 0)]\n",
    "\n",
    "    # get port variables\n",
    "\n",
    "    dict_daily_port_variable = {}\n",
    "\n",
    "    df_long_cur_holding = df_daily_stock_variable[df_daily_stock_variable['cur_hold_qty']>0]\n",
    "    cur_mkv_l = (df_long_cur_holding['cur_hold_qty']*(df_long_cur_holding['close_price'])).sum()\n",
    "    \n",
    "    # Determine Target Long Capital Based on Constraints\n",
    "    dict_daily_port_variable['target_mkv_l'] = int_clip(cur_mkv_l, mkv_range_long[0], mkv_range_long[1])\n",
    "\n",
    "    # define variables\n",
    "\n",
    "    x = cp.Variable(df_daily_stock_variable.shape[0])\n",
    "    l = (df_daily_stock_variable['ind_l'] * df_daily_stock_variable['ind_pool']).values\n",
    "    s = (df_daily_stock_variable['ind_s'] * df_daily_stock_variable['ind_pool']).values\n",
    "    cap_l = dict_daily_port_variable['target_mkv_l']\n",
    "    price = df_daily_stock_variable['close_price'].values\n",
    "    beta = df_daily_stock_variable['beta'].values.round(2)\n",
    "    cur_hold_mkv = df_daily_stock_variable['cur_hold_qty'].values * price\n",
    "    liquid_mkv = (df_daily_stock_variable['liquidity'].values * price).round(2)\n",
    "    target_w = df_daily_stock_variable['tar_w'].values\n",
    "    cap_s = sum(x * s)\n",
    "    target_mkv = target_w * (cap_l * l + cap_s * s)\n",
    "    ls = l + s\n",
    "    not_ls = (1 - l) * (1 - s)\n",
    "    exposure_beta_range = (net_beta_range[0] * cap_l, net_beta_range[1] * cap_l)    \n",
    "    exposure_dollar_range = (net_dollar_range[0] * cap_l, net_dollar_range[1] * cap_l)    \n",
    "    max_dollar_position = max_single_position_w * cap_l\n",
    "    cost = df_daily_stock_variable['cost'].values\n",
    "    borrow_fee = df_daily_stock_variable['borrow_fee'].values\n",
    "    total_cost = (cost + s * borrow_fee)\n",
    "    order_mkv = cp.abs(x - cur_hold_mkv)\n",
    "\n",
    "    # obj \n",
    "\n",
    "    objective = cp.norm(x - target_mkv)\n",
    "    #objective = cp.norm((x - cur_hold_mkv)) # minimize turnover\n",
    "    #objective = cp.sum(order_mkv * total_cost) # minimize trading cost\n",
    "\n",
    "    # constrains\n",
    "\n",
    "    cons_basic = [cp.abs(x) <= max_dollar_position,\n",
    "                  cp.sum(x * l) == cap_l,\n",
    "                  (x * l) >= 1e-9, #1e-9\n",
    "                  (x * s) <= -1e-9, #1e-9\n",
    "                  cp.sum(x * (beta * ls)) <= exposure_beta_range[1],\n",
    "                  cp.sum(x * (beta * ls)) >= exposure_beta_range[0],\n",
    "                  cp.sum(x * ls) <= exposure_dollar_range[1],\n",
    "                  cp.sum(x * ls) >= exposure_dollar_range[0],\n",
    "                  cp.abs(x * not_ls) <= 1e-9,\n",
    "                  ]  \n",
    "\n",
    "    cons_liquid = [cp.abs(x - cur_hold_mkv) <= liquid_mkv]\n",
    "\n",
    "    cons_subind = get_subind_constr_abs (df_daily_stock_variable, \n",
    "                                         x, ls, beta, cap_l, subind_range)\n",
    "    # opt   \n",
    "\n",
    "    try:\n",
    "\n",
    "        prob = cp.Problem(cp.Minimize(objective),  cons_basic + cons_liquid + cons_subind)\n",
    "#         prob = cp.Problem(cp.Minimize(objective),  cons_basic)\n",
    "        prob.solve(verbose = False)  # feastol=1e-10, max_iters = 1000000\n",
    "        print (prob.status)\n",
    "        df_status.loc[_date]['cons1'] = prob.status\n",
    "\n",
    "        if prob.status == 'optimal': opt_value = x.value\n",
    "\n",
    "        if prob.status == 'infeasible' or prob.status =='optimal_inaccurate':\n",
    "\n",
    "            prob = cp.Problem(cp.Minimize(objective),  cons_basic + cons_subind)\n",
    "            prob.solve(verbose = False)   # feastol=1e-10\n",
    "            print (prob.status)\n",
    "            df_status.loc[_date]['cons2'] = prob.status\n",
    "            if prob.status == 'optimal': opt_value = x.value\n",
    "\n",
    "        if prob.status == 'infeasible' or prob.status =='optimal_inaccurate':\n",
    "\n",
    "            prob = cp.Problem(cp.Minimize(objective),  cons_basic)\n",
    "            prob.solve(verbose = False)   # feastol=1e-10\n",
    "            print (prob.status)\n",
    "            df_status.loc[_date]['cons3'] = prob.status\n",
    "            if prob.status == 'optimal': opt_value = x.value\n",
    "\n",
    "        if prob.status == 'infeasible' or prob.status =='optimal_inaccurate':\n",
    "\n",
    "            beta_l = sum(cur_hold_mkv * l * beta) / sum(cur_hold_mkv * l)\n",
    "            beta_s = sum(cur_hold_mkv * s * beta) / sum(cur_hold_mkv * s)\n",
    "            cap_s = abs((cap_l * beta_l - 0.2 * cap_l) / beta_s)\n",
    "            target_mkv = target_w * (cap_l * l + cap_s * s)\n",
    "            opt_value = target_mkv\n",
    "\n",
    "    except:\n",
    "\n",
    "        beta_l = sum(cur_hold_mkv * l * beta) / sum(cur_hold_mkv * l)\n",
    "        beta_s = sum(cur_hold_mkv * s * beta) / sum(cur_hold_mkv * s)\n",
    "        cap_s = abs((cap_l * beta_l - 0.2 * cap_l) / beta_s)\n",
    "        target_mkv = target_w * (cap_l * l + cap_s * s)\n",
    "        opt_value = target_mkv\n",
    "\n",
    "    # generate orders\n",
    "\n",
    "    df_daily_stock_variable['date'] = _date\n",
    "    df_daily_stock_variable.index.names = ['Ticker']\n",
    "    df_daily_stock_variable['capital_opt'] = opt_value\n",
    "    df_daily_stock_variable['capital_opt'] = (df_daily_stock_variable['capital_opt'] * df_daily_stock_variable['ind_l']).clip(0, np.inf) \\\n",
    "                                    + (df_daily_stock_variable['capital_opt'] * df_daily_stock_variable['ind_s']).clip(-np.inf, 0)\n",
    "\n",
    "    # generate orders\n",
    "\n",
    "    df_daily_stock_variable['target_holding_qty'] = (df_daily_stock_variable['capital_opt'] \\\n",
    "                                            / df_daily_stock_variable['close_price']).fillna(0)\n",
    "    df_daily_stock_variable['target_holding_qty'].loc[(~np.isfinite(df_daily_stock_variable['target_holding_qty']))] = 0\n",
    "    df_daily_stock_variable['target_holding_qty'] = df_daily_stock_variable['target_holding_qty'].apply(np.int)\n",
    "\n",
    "    df_daily_stock_variable['order_qty'] = (df_daily_stock_variable['target_holding_qty'] - df_daily_stock_variable['cur_hold_qty'])\n",
    "\n",
    "    df_daily_stock_variable['holding_mkv'] = (df_daily_stock_variable['order_qty'] + df_daily_stock_variable['cur_hold_qty']) \\\n",
    "                                            * df_daily_stock_variable['close_price']\n",
    "    df_daily_stock_variable['holding_mkv_beta_adj'] = df_daily_stock_variable['holding_mkv'] \\\n",
    "                                                    * df_daily_stock_variable['beta']\n",
    "\n",
    "    df_daily_stock_variable['side'] = df_daily_stock_variable['capital_opt'].apply(lambda x: 'Long' if x > 0 \n",
    "                                                                                    else 'Short' if x <= 0\n",
    "                                                                                    else '')                \n",
    "\n",
    "    # save orders\n",
    "    df_daily_stock_variable['date'] = _date\n",
    "\n",
    "    dict_daily_order[_date] =  df_daily_stock_variable    \n",
    "\n",
    "    dict_daily_order['status'] = df_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save optimized portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_output = dir_farm2 +'/Research/PORTFOLIO_OPTIMIZATION/Tradelog_2016_2018/'\n",
    "list_dates = [x for x in dict_daily_order.keys() if '-' in x]\n",
    "dict_date = dict(zip(list_dates[:-1], list_dates[1:]))\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "for _date in list_dates:\n",
    "    \n",
    "    df_temp = dict_daily_order[_date].reset_index()\n",
    "    df_all = df_all.append(df_temp)\n",
    "\n",
    "df_all['exit_date'] = df_all['date'].map(dict_date)\n",
    "df_close_stack =dict_data['_closeF'].fillna(method='ffill', limit=60).stack()\n",
    "df_all = df_all.set_index(['exit_date','Ticker'])\n",
    "df_all['exit_price'] = df_close_stack\n",
    "df_all.reset_index().to_csv(dir_output + 'orders/SI_15days_1000L_1000S_BASE_DollarEXP_0.3_0.6_2016_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
